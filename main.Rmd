---
title: "Calibration Experiment"
output:
  html_notebook: default
  pdf_document: default
---
[comment]: <> (Markdown Section. R code un {chunks})
```{r include=FALSE, echo=FALSE}
# dependencies
if(!require(psych)){install.packages('psych')} # for describe()
library(psych)

# Companion to Applied Regression CAR (who decides these horrible names?)
# for qqPlot,
# leveneTest for homogeneity of variance (for t-test) -> wrong, this is for independent samples t-test comparison (not our case)
if(!require(car)){install.packages('car')} 
library(car)

# plot using a grid
if(!require(grid)){install.packages('grid')}
library(grid)

if(!require(gridExtra)){install.packages('gridExtra')}
library(gridExtra)

# for paired plots and other ready plots
if(!require(ggpubr)){install.packages('ggpubr')}
library(ggpubr)

if(!require(tidyverse)){install.packages('tidyverse')}
library(tidyverse)

# to find what's the data distribution
if(!require(fitdistrplus)){install.packages('fitdistrplus')}
library(fitdistrplus)
if(!require(logspline)){install.packages('logspline')}
library(logspline)

# for identify_outliers and others
if(!require(rstatix)){install.packages('rstatix')}
library(rstatix)

if(!require(afex)){install.packages('afex')}
library("afex")  

```

```{r include=FALSE, echo=FALSE}
# set up working directory and read raw input files
sprintf("current working directory = %s", getwd())
results_directory = "results"
questionnaire_raw_filename = paste(results_directory, "0_PostQuestionnaire.csv", sep="/")
questionnaire_raw <- read.csv(questionnaire_raw_filename, na.strings = c("-"))

# note:
# R renamed column names starting with a number to X<number>
```

## Part 1 SUS Questionnaire

```{r include=FALSE, echo=FALSE}
# Let's declare the name of the columns and whether they are a positive or negative question
keyboard_columns = c(
  'X1_Complex_Keyboard',
  'X2_Easy_Keyboard',
  'X3_learn_quickly_Keyboard',
  'X4_Cumbersome_Keyboard',
  'X5_Confident_Keyboard',
  'X6_Learn_Lots_Keyboard',
  'X7_Fast_Keyboard',
  'X8_Accurate_Keyboard'
)


slider_columns = c(
  'X1_Complex_Slider',
  'X2_Easy_Slider',
  'X3_learn_quickly_Slider',
  'X4_Cumbersome_Slider',
  'X5_Confident_Slider',
  'X6_Learn_Lots_Slider',
  'X7_Fast_Slider',
  'X8_Accurate_Slider'
)

positivity = c(
    'X1_Complex' = F,
    'X2_Easy' = T,
    'X3_learn_quickly' = T,
    'X4_Cumbersome' = F,
    'X5_Confident' = T,
    'X6_Learn_Lots' = F,
    'X7_Fast' = T,
    'X8_Accurate' = T
)

get_base_name <- function(col_name)
{
  print(col_name)
  print(typeof(col_name))
  base = substr(col_name, 1, rfind(col_name, '_')-1)
  print(base)
  return(base)
}

get_base_name_cols <- function(col_names)
{
  return(lapply(col_names, get_base_name))
}
```


```{r include=FALSE, echo=FALSE}
ids = seq.int(nrow(questionnaire_raw))

keyboard_answers <- questionnaire_raw %>%
  select(ends_with("Keyboard"))

slider_answers <- questionnaire_raw %>%
  select(ends_with("Slider"))

keyboard_answers$method <- "Keyboard"
keyboard_answers$participant <- ids

slider_answers$method <- "Slider"
slider_answers$participant <- ids

keyboard_answers <- keyboard_answers %>%
  rename(
    X1_Complex = X1_Complex_Keyboard,
    X2_Easy = X2_Easy_Keyboard,
    X3_learn_quickly = X3_learn_quickly_Keyboard,
    X4_Cumbersome = X4_Cumbersome_Keyboard,
    X5_Confident = X5_Confident_Keyboard,
    X6_Learn_Lots = X6_Learn_Lots_Keyboard,
    X7_Fast = X7_Fast_Keyboard,
    X8_Accurate = X8_Accurate_Keyboard
)

slider_answers <- slider_answers %>%
  rename(
    X1_Complex = X1_Complex_Slider,
    X2_Easy = X2_Easy_Slider,
    X3_learn_quickly = X3_learn_quickly_Slider,
    X4_Cumbersome = X4_Cumbersome_Slider,
    X5_Confident = X5_Confident_Slider,
    X6_Learn_Lots = X6_Learn_Lots_Slider,
    X7_Fast = X7_Fast_Slider,
    X8_Accurate = X8_Accurate_Slider
)

sus_df <- bind_rows(keyboard_answers, slider_answers)

# let's invert the score of the negative questions
sus_df <- sus_df %>%
  mutate(
    X1_Complex = 4 - X1_Complex,
    X4_Cumbersome = 4 - X4_Cumbersome,
    X6_Learn_Lots = 4 - X6_Learn_Lots
  )

# 5 positive questions and 3 negative
# max score 8 x 4 = 32 -> scale_factor = 3.125
#scale_factor = 3.125
nr_questions <- 8 # change here depending on how many questions we want to consider
max_score_val <- 4
scale_factor <- 100 / (nr_questions * max_score_val)

sus_df <- sus_df %>%
  mutate(score = rowSums(.[1:nr_questions]) * scale_factor)

```


```{r  include=FALSE, echo=FALSE}
# Let's plot the distribution of the scores
p1 <- ggplot(sus_df, aes(score)) + geom_histogram()
p1 <- ggplot(sus_df, aes(score)) + geom_histogram()
p1 <- ggplot(sus_df, aes(score)) + geom_histogram()

sus_keyboard <- sus_df %>%
  filter(method == "Keyboard")

sus_slider <- sus_df %>%
  filter(method == "Slider")

p1_global <- ggplot(sus_df, aes(score)) + geom_histogram()
p1_keyboard <- ggplot(sus_keyboard, aes(score)) + geom_histogram()
p1_slider <- ggplot(sus_slider, aes(score)) + geom_histogram()

grid.arrange(p1_global, p1_keyboard, p1_slider, nrow=1)
```
We calculate a SUS-like score for a system where participants need to calibrate the intensity.
In one condition, participants use a keyboard and in the other one they do it in VR using an avatar
interacting with a virtual slider. We get the following distribution of scores for each conditions

```{r echo=FALSE}
ggdensity(sus_df,
  x = "score", # variable of interest
  add = "mean",# add vertical line for mean
  rug = TRUE, # add "rugs" to display the density of the values along the axis
  color = "method", fill = "method",
  palette = c("#00AFBB", "#E7B800"))
```
We attempt to find if the difference of the scores between the conditions is statistically significant
by means of a paired T-test.

We need to verify the assumption that the observation (the difference between two set of values) is continuous and approximately normally distributed.

The distribution of the difference is as follows
```{r echo=FALSE}
difference_t_test = sus_keyboard$score - sus_slider$score
hist(difference_t_test,
     main="Distribution of the difference of SUS Scores",
     xlab="difference of scores")
```
The previous distribution doesn't look normal but we can verify this by performing a Shapiro-test
and inspecting a Q-Q plot
```{r echo=FALSE}
shapiro_test(difference_t_test)
#print(shapiro_test(difference_t_test)) #0.023 -> not normal
#qq_data <- as.data.frame(difference_t_test)
qqplot_result <- qqPlot(difference_t_test)
```
A p-value of 0.02 makes us reject the null hypothesis (normality of the data)

Given that we do not fulfill the assumptions, we perform a non-parametric test to see
if the difference between the scores is statistically significant

We perform a two-sample Wilcoxon-test (also known as Mann-Whitney-test)
```{r echo=FALSE}
#sus_keyboard$score
#sus_slider$score
# the scores for each condition are the same in 2 out of 16 participants.
difference <- sus_keyboard$score - sus_slider$score
# difference
test_result <- wilcox.test(sus_keyboard$score, sus_slider$score, paired = TRUE, alternative = "two.sided")
print(test_result)
```
Wilcoxon-test warnings us about the similarity of the scores.
In anyway, we obtain a p-value of 0.2 which tells us there is no evidence to reject the null hypothesis.

Non-parametric tests have less statistical power than parametric ones.
We perform a paired T-test just to make sure we are not missing finding an effect.

```{r echo=FALSE}
# for paired t-test, differences need to be normally distributed
#str(qq_data)
#p1<-ggplot(qq_data, aes(sample=difference_t_test)) + stat_qq() + stat_qq_line()
#p1
# qqPlot from car package draws also surrounding area

t.test(sus_keyboard$score, sus_slider$score, paired = TRUE, alternative = "two.sided") #p-value 0.1642
```
Once again we get the same result that there is no statistically significant difference between the scores.
This was kind of visible from the first plot showing the histogram of the scores and this is also reinforced by seeing the distribution of the scores using box and violin plots

```{r echo=FALSE}
a1 <- aov_ez("participant", "score", sus_df,
             between = NULL,
             within = c("method"),
             anova_table = list(es = "pes"))

## Check the normality of the residuals
# in this case, the residuals are not normal -> let's do a non-parametric test
#residuals <- a1$lm$residuals
#shapiro.test(residuals)
#qqPlot(residuals)

# boxplot to see outliers
p1 <- afex_plot(a1, x = "method", error = "within", 
                 mapping = c("linetype", "shape", "fill"),
                 data_geom = ggplot2::geom_boxplot, 
                 data_arg = list(width = 0.5))

# violin plot to see distribution
p2 <- afex_plot(a1, x = "method", error = "within", 
                mapping = c("linetype", "shape", "fill"),
                data_geom = ggplot2::geom_violin, 
                data_arg = list(width = 0.5))

grid.arrange(p1, p2, nrow=1, top=textGrob("Distribution of SUS scores (box & violin plots)"))

```

Given that the overall scores were similar, we start to analyze differences question by question.
The SUS-like questionnaire was composed of 8 Likert-scale questions with answers in the range [0, 4] (Strongly Disagree, Strongly Agree).

We perform a two-sample Wilcoxon-test for each of the 8 questions and we get the following p-values:
```{r echo=FALSE, warning=FALSE}
#set up empty variable to store all simulated p-values
nr_questions <- 8
p <-numeric(nr_questions) 

for (question_id in 1: nr_questions)
{
  keyboard_answers <- sus_keyboard[question_id]
  slider_answers <- sus_slider[question_id]
  
  keyboard_answers <- as.numeric(unlist(keyboard_answers))
  slider_answers <- as.numeric(unlist(slider_answers))
  
  differences <- keyboard_answers - slider_answers
  #print(differences)
  #print(paste("non-parametric test for question", question_id))

  test_result <- wilcox.test(keyboard_answers, slider_answers, paired = TRUE, alternative = "two.sided")
  p[question_id] <- test_result$p.value
}
print(p)
#q1_keyboard = sus_keyboard$X1_Complex

```
Once again we get a warning telling us the similarity between the scores among conditions (hidden in the notebook).
We can see most of the p-values are not significant except for the question number 7 which was regarding the speed of the method.

```{r echo=FALSE}
question_seven <- sus_df %>%
  select(X7_Fast, method)

#speed_keyboard <- sus_keyboard[7]
#speed_slider <- sus_slider[7]
#keyboard_answers <- as.numeric(unlist(speed_keyboard))
#slider_answers <- as.numeric(unlist(speed_slider))
ggplot(question_seven, aes(X7_Fast, fill=method)) + geom_bar(position="dodge") + xlab("Speed Score") 
```
Here we can see most of the participants graded the keyboard condition with a lower score compared to the calibration in VR using the slider.


## Part 2 Analysis of In-Experiment Measured Data

```{r include=FALSE, echo=FALSE}
input_files <- list.files(
  path = results_directory,
  pattern = "*CalibrationMainTableTrials.csv",
  full.names = TRUE)

first_file <- TRUE
for (input_file in input_files)
{
  #print(input_file)
  first_index <- str_locate(input_file, "/")[1]
  last_index <- str_locate(input_file, "_")[1]
  participant_id <- strtoi(str_sub(input_file, first_index+1, last_index-1))
  # add participant id and add block order info
  partial_df <- read_delim(input_file, delim=';', skip=1, show_col_types = FALSE) %>%
    add_column(participant = participant_id) %>%
    mutate(repetition = if_else(trial == 1 | trial == 2, 1, 2)) %>%
    mutate(first = if_else(trial == 1 | trial == 3, TRUE, FALSE))
  
  if (first_file)
  {
    measured_data <- partial_df
    first_file <- FALSE
  } else {
    measured_data <- bind_rows(measured_data, partial_df)
  }
}

measured_data <- measured_data %>%
  rename(
    firstContactEndTrialElapsed = `firstContact-endTrial-elapsedTime`,
    firstContactLastContactElapsed = `firstContact-lastContact-elapsedTime`,
    firstContactLastSensationElapsed = `firstContact-lastSensation-elapsedTime`,)
measured_data$condition <- factor(measured_data$condition)

rm(partial_df)
```


```{r include=FALSE}
str(measured_data)
```
```{r include=FALSE}
measured_data %>%
  select(sensation, discomfort, dynamicRange, firstContactEndTrialElapsed, firstContactLastContactElapsed, firstContactLastSensationElapsed) %>%
  describe()

measured_data <- measured_data %>%
    #add_column(logFirstContactEndTrialElapsed = log10(.$firstContactEndTrialElapsed))
    mutate(logFirstContactEndTrialElapsed = log10(firstContactEndTrialElapsed))

measured_data <- measured_data %>%
    mutate(logDynamicRange = log10(dynamicRange))
#describe(measured_data)
```

```{r  include=FALSE}
# create grouped dataframe
grouped_data <-  measured_data %>%
  group_by(participant, condition) %>%
  summarise_at(
    vars(firstContactEndTrialElapsed, logFirstContactEndTrialElapsed, dynamicRange, logDynamicRange),
    mean)

grouped_data_keyboard <- grouped_data %>%
  filter(condition == "Keyboard")

grouped_data_slider <- grouped_data %>%
  filter(condition == "Slider")
```


As objective metrics, we recorded the calibration values (sensation and discomfort thresholds) obtained from the calibrations using each of the two proposed methods (keyboard, slider). Additionally, we recorded how long it took to calibrate using each of the methods.

To be more precise, we recorded three temporal metrics:

1. time since the beginning of the calibration until the last adjustment (interaction time)
2. time since the beginning of the calibration until the stimulation was turned off (sensation time)
3. time since the beginning of the calibration until the end of the calibration (calibration time)

In this section we analyze the resulting dynamic range (difference between the discomfort and sensation threshold) and the calibration time.

We start by seeing the distribution of the overall recorded values.
```{r echo=FALSE, warning=FALSE}
default_nr_bins <- 30
p1 <- ggplot(measured_data, aes(sensation)) + geom_histogram(bins=default_nr_bins) 
p2 <- ggplot(measured_data, aes(discomfort)) + geom_histogram(bins=default_nr_bins)
p3 <- ggplot(measured_data, aes(dynamicRange)) + geom_histogram(bins=default_nr_bins) + xlab("dynamic range")
p4 <- ggplot(measured_data, aes(firstContactLastContactElapsed)) + geom_histogram(bins=default_nr_bins) + xlab("interaction time")
p5 <- ggplot(measured_data, aes(firstContactLastSensationElapsed)) + geom_histogram(bins=default_nr_bins) + xlab("sensation time")
p6 <- ggplot(measured_data, aes(firstContactEndTrialElapsed)) + geom_histogram(bins=default_nr_bins) + xlab("calibration time")
#p2<-ggplot(simulatedData, aes(d)) +
#  geom_histogram(binwidth = 4)
#p3<-qplot(simulatedData$d,geom="boxplot")
#p4<-ggplot(simulatedData, aes(sample = d)) + stat_qq() + stat_qq_line()

#grid.arrange(p1, p2, p3, p4)
#grid.arrange(p1, p2, p3, p4, p5, p6, nrow=1)
grid.arrange(p1, p2, p3, p4, p5, p6, nrow=2)


#hist(measured_data$firstContactEndTrialElapsed)
#boxplot(measured_data$`firstContact-endTrial-elapsedTime` ~ measured_data$condition)
```
By comparing the last three plots, we can see the values and the distribution of the temporal variables are quite similar so we will analyze just the calibration time as a representative of the three.

For the calibration variables, we will focus only in the calibration dynamic range.

Our hypotheses are that the calibration in VR is faster and produces exactly the same calibration dynamic range
as the keyboard condition, i.e participants are capable of setting the exact same values but faster.

Let's start the analysis with the calibration time variable. Let's see the distribution of the variable per condition
```{r echo=FALSE}
ggdensity(grouped_data,
  x = "firstContactEndTrialElapsed", # variable of interest
  add = "mean",# add vertical line for mean
  rug = TRUE, # add "rugs" to display the density of the values along the axis
  color = "condition", fill = "condition",
  palette = c("#00AFBB", "#E7B800"),
  xlab="calibration time")
```
We see that the average calibration time using the keyboard method is longer than the slider method.
We would like to test if that difference is statistically significant. We would like to try a parametric test but we need to know what the distribution of our data is.

We analyze the skewness-kurtosis of our data using a Cullen and Frey graph

```{r echo=FALSE}
descdist(measured_data$firstContactEndTrialElapsed, discrete = FALSE)
```
Based on the previous graph, it seems our data follows some sort of log-normal distribution.

We can reassure this by making a Q-Q plot against that distribution
```{r  echo=FALSE}
#scaled_data <- (measured_data$firstContactEndTrialElapsed - min(measured_data$firstContactEndTrialElapsed) + 0.001) / (max(measured_data$firstContactEndTrialElapsed) - min(measured_data$firstContactEndTrialElapsed) + 0.002)
#fit.beta <- fitdist(scaled_data, "beta")
#p1 <- plot(fit.beta)

#fit.gamma <- fitdist(measured_data$firstContactEndTrialElapsed, "gamma")
#p2 <- plot(fit.gamma)

# r internal function is called lnorm and not lognormal (stats package)
fit.lognormal <- fitdist(measured_data$firstContactEndTrialElapsed, "lnorm")
p3 <- plot(fit.lognormal)

#p1

#p2

#p3
```

If our data is effectively log-normally distributed, then the logarithm of it will follow a normal distribution.

```{r echo=FALSE}
p1 <- ggplot(measured_data, aes(logFirstContactEndTrialElapsed)) + geom_histogram(bins=default_nr_bins) + xlab("log(calibration time)")
plot(p1)


fit.normal <- fitdist(measured_data$logFirstContactEndTrialElapsed, "norm")
p2 <- plot(fit.normal)

#p2


```

This is confirmed using a Shapiro-test for normality having strong evidence for that.
```{r echo=FALSE}
result_test <- shapiro_test(measured_data$logFirstContactEndTrialElapsed) # 0.95 -> data is normal
print(result_test)
```
We will perform a paired T-test to compare the means between the two conditions.
We will start by inspecting the distribution of the difference between observations.
This is going to be performed in our transformed data (logarithmic).
```{r include=FALSE}
# Paired T-Test expect the difference between the two observations to be normally distributed
difference_t_test <- grouped_data_keyboard$logFirstContactEndTrialElapsed - grouped_data_slider$logFirstContactEndTrialElapsed

```

```{r echo = FALSE}
hist(difference_t_test, xlab="differences of log(calibration time)", main = "distribution of the differences")
shapiro_test(difference_t_test) #0.6265 -> normal
#qq_data <- as.data.frame(difference_t_test)
#str(qq_data)
#p1<-ggplot(qq_data, aes(sample=difference_t_test)) + stat_qq() + stat_qq_line()
#p1

# qqPlot from car package draws also surrounding area
result_qq <-qqPlot(difference_t_test)
```
We have evidence that our data respects the normality assumption, so we can perform a paired T-test without any problem.

```{r echo = FALSE}
#within-subjects
# we got p-value 0.008
t.test(grouped_data_keyboard$logFirstContactEndTrialElapsed, grouped_data_slider$logFirstContactEndTrialElapsed, paired = TRUE, alternative = "two.sided")
```
We found that the difference of the means is statistically significant. This can also be observed visually by plotting the observations of the calibration time per participants.

```{r echo = FALSE}
#boxplot(grouped_data$logFirstContactEndTrialElapsed~grouped_data$condition)
ggpaired(data = grouped_data, x = "condition" , y = "logFirstContactEndTrialElapsed", id="participant", fill="condition") + ylab("log(calibraion time)")
#grid.arrange(p1, p2, nrow=1)
```
We can appreciate in the previous plot that most of the connecting lines go down, indicating that it took less time to calibrate using the slider for most of the participants.
```{r include = FALSE}
# manual calculation of t-statistics
m <- mean(difference_t_test)
s <- sd(difference_t_test) # sample standard deviation
t <- m / (s / sqrt(length(difference_t_test)))
```

We can perform a non-parametric test (two-sample Wilcoxon) in the non-transformed variable just to confirm our hypothesis.

```{r echo =FALSE}
wilcox.test(grouped_data_keyboard$firstContactEndTrialElapsed, grouped_data_slider$firstContactEndTrialElapsed, paired = TRUE, alternative = "two.sided") # 0.02
```
And once again we confirm the evidence with a p-value of 0.02.

We get a similar result if we run a 1-way ANOVA with repeated measures for the non-transformed data. For this test, we need to verify that the distribution of the residuals is normally distributed.
```{r echo = FALSE}
a1 <- aov_ez("participant", "firstContactEndTrialElapsed", grouped_data,
             between = NULL,
             within = c("condition"),
             anova_table = list(es = "pes"))

## Check the normality of the residuals
residuals <- a1$lm$residuals
shapiro.test(residuals)
result_qq <- qqPlot(residuals)
```
Our residuals are normally distributed and we can now perform the ANOVA test.
```{r echo = FALSE}
## Anova table
a1$anova_table # 0.0458
```
In this case we get a p-value of 0.045 when comparing directly the calibration time using ANOVA.

Let's end the analysis of the calibration time by plotting the distribution of the data using a box and a violin plots.

```{r echo=FALSE}
#anova_one_way_repeated <- aov(logFirstContactEndTrialElapsed~condition + Error(participant/condition), data = #grouped_data)
#summary(anova_one_way_repeated)



#t.test(grouped_data_keyboard$firstContactEndTrialElapsed, grouped_data_slider$firstContactEndTrialElapsed, paired = TRUE, alternative = "two.sided")
p1 <- afex_plot(a1, x = "condition", error = "within", 
                mapping = c("linetype", "shape", "fill"),
                data_geom = ggplot2::geom_boxplot, 
                data_arg = list(width = 0.5))
p2 <- afex_plot(a1, x = "condition", error = "within", 
                mapping = c("linetype", "shape", "fill"),
                data_geom = ggplot2::geom_violin, 
                data_arg = list(width = 0.5)) 

grid.arrange(p1, p2, nrow=1, top=textGrob("Distribution of calibration time (box & violin plots)"))

```
Here we can also appreciate how much more variability there is under the keyboard condition and if it weren't by an observation marked as outlier in the slider condition, this difference would be greater.

Let's now perform a similar analysis for the size of the dynamic range. Let's remember that our hypothesis is that there won't be any significant difference between both methods.

We will start by seeing the distribution per condition.
And for the dynamic range
```{r echo = FALSE}
ggdensity(grouped_data,
  x = "dynamicRange", # variable of interest
  add = "mean",# add vertical line for mean
  rug = TRUE, # add "rugs" to display the density of the values along the axis
  color = "condition", fill = "condition",
  palette = c("#00AFBB", "#E7B800"), xlab = "dynamic range")
```
The values look quite similar and the distributions have a considerable overlap.

As in the previous analysis, we check for the normality of the data.
```{r echo = FALSE}
difference_t_test <- grouped_data_keyboard$dynamicRange - grouped_data_slider$dynamicRange
hist(difference_t_test, xlab="differences of dyanmic range", main = "distribution of the differences")
shapiro_test(difference_t_test) #0.6265 -> normal
result_qq <-qqPlot(difference_t_test)
```
We see clearly that neither the dynamic range data nor the difference between the conditions are normally distributed.

We proceed using the non-parametric two-sample Wilcoxon test.
```{r echo = FALSE}
wilcox.test(grouped_data_keyboard$dynamicRange, grouped_data_slider$dynamicRange, paired = TRUE, alternative = "two.sided") # 0.9
```

The non-parametric test tells us there is no statistically significant difference (it even warnings us about the similarity of the observations)

We will try in any way running a 1-way repeated measure ANOVA so let's check the assumption of normality for the residuals
```{r echo=FALS}
a1 <- aov_ez("participant", "dynamicRange", grouped_data,
             between = NULL,
             within = c("condition"),
             anova_table = list(es = "pes"))

## Check the normality of the residuals
residuals <- a1$lm$residuals
shapiro.test(residuals)
qqPlot(residuals)
```
The distribution of the residuals is not normal and the p-value reported by ANOVA also says there is no significant difference.
```{r echo = FALSE}
## Anova table
a1$anova_table #p-value 0.54
```

All of this is reinforced by inspecting the distribution of the observations
```{r echo=FALSE}
# boxplot to see outliers
p1 <- afex_plot(a1, x = "condition", error = "within", 
                 mapping = c("linetype", "shape", "fill"),
                 data_geom = ggplot2::geom_boxplot, 
                 data_arg = list(width = 0.5))

# violin plot to see distribution
p2 <- afex_plot(a1, x = "condition", error = "within", 
                mapping = c("linetype", "shape", "fill"),
                data_geom = ggplot2::geom_violin, 
                data_arg = list(width = 0.5))

grid.arrange(p1, p2, nrow=1)
```
We see how they are practically identical, therefore our hypothesis seems right.

The observations look pretty much one-to-one, i.e. for a given participant, its dynamic range didn't change
based on the keyboard/slider condition
```{r echo = FALSE}
ggpaired(data = grouped_data, x = "condition" , y = "dynamicRange", id="participant", fill="condition")
```






```{r include = FALSE, echo=FALSE}
descdist(measured_data$dynamicRange, discrete = FALSE)
```


```{r  include=FALSE, echo=FALSE}
fit.lognormal <- fitdist(measured_data$dynamicRange, "lnorm")
p3 <- plot(fit.lognormal)


fit.normal <- fitdist(measured_data$logDynamicRange, "norm")
p2 <- plot(fit.normal)

result_test <- shapiro_test(measured_data$logDynamicRange) # 0.01 -> data is normal
print(result_test)
```


```{r include=FALSE, echo=FALSE}
# OUTLIERS ANALYSIS. NOT INCLUDED

# Let's try to find the outliers regarding the calibration elapsed time
nr_participants = n_distinct(measured_data$participant)

for (id in 1: nr_participants)
{
  participant_data <- measured_data %>%
  filter(participant == id)
  #ggtitle(paste("participant", id))
  p1 <- ggplot(participant_data, aes(firstContactEndTrialElapsed)) + geom_boxplot() + coord_flip()
  p2 <- ggplot(participant_data, aes(x=trial, y=firstContactEndTrialElapsed)) + geom_point()
  grid.arrange(p1, p2, nrow=1, top=textGrob(paste("participant ", id)))
}

# Let's try to find the outliers regarding the calibration elapsed time

# 4 outliers, one of them extreme.
# this were obtained using all datapoints and not using a "within-participant" comparison.
# 1, 15, 9 outliers
# 2 extreme outlier
measured_data_outliers <- identify_outliers(measured_data, variable="firstContactEndTrialElapsed")

# add outlier info to the main table
measured_data$outlier <- is_outlier(measured_data$firstContactEndTrialElapsed)
measured_data$extreme <- is_extreme(measured_data$firstContactEndTrialElapsed)

# show ""outliers""
#measured_data$participant[which(measured_data$extreme)]

# same as subset(df, filter, vars to show)
subset(measured_data, extreme, c(participant, trial, condition))

```

